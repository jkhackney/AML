{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Advanced Machine Learning Computer Vision Project: Summary\n",
    "### Goal: Auto-analysis of the color of half a dozen photos of thousands of ice crystals in snow to determine the angle of the crystals\n",
    "\n",
    "\n",
    "### Challenge 1: Must assign the same crystal ID to each crystal in each photo. Some crystals disappear (go dark) in some photos\n",
    "### Challenge 2: Only 3 samples of crystals consisting of 6-8 images each are available; is training an ML model possible?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Procedure:\n",
    "\n",
    "1. Align the photos: rotation (PIL; Irfanview) and registration (DRMIME_2D from ZeroCostDL4Mic)\n",
    "\n",
    "2. Identify crystals of ice across the images with Machine Learning: two methods were tested\n",
    "    - Method 1: Apply Holistically Nested Edge Detection (HED)  to find edges of the crystals  (from Xie at UCSD: a UNET neural network implemented in Caffe)\n",
    "    - Then apply Connected Component Analysis (OpenCV) to transform edges into regions of interest (segmentation)\n",
    "    <br><br>\n",
    "    - Method 2: cellpose segmentation for microbiology (cytoplasm model) from ZeroCostDL4Mic\n",
    "    <br><br>\n",
    "\n",
    "3. Prepare a dataframe of all crystals in all photos with \"regionsprops\" function in skimage\n",
    "\n",
    "4. Compare the crystals' refraction color to a color chart to determine wavelength shift from light refraction and to calculate the crystals' angle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Issues encountered in the project\n",
    "\n",
    "### ML is pretty good at registering images with linear displacements but not rotation\n",
    "### I didn't quickly understand pytorchvision methods to detect edges and segment objects\n",
    "### It was much quicker to apply out-of-the-box pre-trained models for segmentation, edge-detection and component analysis\n",
    "### These models are sensitive to the contrast of the input image (gamma adustments)\n",
    "### I learned many ways to manipulate image pixels\n",
    "### Care must be taken with the different index conventions for color images: pytorch tensor, jpg-tiff, Open CV BGR and YX, PIL and matplotlib!!\n",
    "### RGB and wavelength of light is a complicated nonlinear mapping. Comparing colors across images requires calibrating the RGB conversion of the images during the data gathering process!!\n",
    "### OpenCV is tricky: \"regioprops\" adds a zero-region for the background and the index of each segment (and number of segments) increases by one relative to the filter used in the component threshold layer !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v3 as iio\n",
    "from PIL import Image\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
    "local_folder = \"../\"\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/guiwitz/MLCV/main/notebooks/check_colab.py', 'check_colab.py')\n",
    "from check_colab import set_datapath\n",
    "colab, datapath = set_datapath(local_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up experiment series and base directory to work in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = \"data/SnowProject/all/\"\n",
    "experiment = \"L7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datapath / archive / experiment / \"P2024894.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlepage=plt.imread(\"../project/Colbeck_Equation_slide.jpg\")\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(titlepage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow crystals (Type 1 ice formed at atmospheric pressure) are anisotropic: stacked sheets of tetrahedrons (H2O)\n",
    "## The sheets form an optical C-axis across which the crystals also shear more easily.\n",
    "## Thus optical and mechanical characteristics coincide, providing a possible observation window into fracture mechanics\n",
    "## Can we find evidence of crystals rolling until they shear along the C-axis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldwork=plt.imread(\"../project/track_sample01_slide.jpg\")\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(fieldwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A sample of the ice glaze of a ski track under crossed polaroids.\n",
    "# The sample was prepared by freezing it to a glass slide and grinding the snow off the back on a precision milling machine\n",
    "# Photographed between polaroid filters on 35mm slide film at -20C\n",
    "# Converted into RGB space with a digital SLR camera\n",
    "# Width of the original photograph is 36mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_scale=plt.imread(\"../project/Img0_with_scale_slide.jpg\")\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(sample_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birefringent refraction\n",
    "- The speed of light in the crystal is maximum along the \"ordinary\" C-axis and less along the extraordinary axes.\n",
    "\n",
    "https://www.microscopyu.com/techniques/polarized-light/principles-of-birefringence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biref_waves=plt.imread(\"../project/birefgringent_wavefronts_01.jpg\")\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.imshow(biref_waves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The polarizer\n",
    "https://www.microscopyu.com/techniques/polarized-light/principles-of-birefringence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biref_polaroids=plt.imread(\"../project/birefringent_polaroids_slide.jpg\")\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(biref_polaroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The crystal color is black when the azimuth angle of the C-axis or extraordinary beam is aligned with a polariod\n",
    "# The color is brightest when the azimuth angle of the C-axis or extraordinary beam is aligned 45 deg to a polaroid\n",
    "# The color at this azimuth uniquely corresponds to the maximum wavefront path difference (nm)\n",
    "# This path length difference allows calculation of the polar angle of the crystal relative to the light beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = datapath / archive / experiment\n",
    "print(basepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all the raw data files (JPG) from the base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "im_list =[] # list of raw data images\n",
    "\n",
    "# iterate through files and add to list of images\n",
    "for file in os.listdir(os.path.join(basepath, \"resized\")):\n",
    "    if file.endswith(\"jpg\"):\n",
    "        file_path=basepath / \"resized\" / file\n",
    "        print(file_path)\n",
    "    #load_image(file_path)\n",
    "        im_list.append(Image.open(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above uses JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(im_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "backimg=im_list[0]\n",
    "ax.imshow(im_list[0])\n",
    "img=plt.imshow(backimg)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    backimg=im_list[i]\n",
    "    imgnew=img.set_data(backimg)\n",
    "\n",
    " \n",
    "    ax.imshow(backimg)\n",
    "\n",
    "    return(img)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal is to identify each crystal and trace its color/intensity shifts through the angle of azimuthal rotation.\n",
    "\n",
    "### There is one image at each azimuth angle, approx. 6-7 images per snow sample\n",
    "### The same crystal appears in each photo but it is a different color and at different coordinates!\n",
    "### It needs to retain the same ID number in each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "crystal_edges_hand = cv2.imread(str(datapath / \"project/L7_1993_crop.jpg\"))\n",
    "fig, ax = plt.subplots(1,1,figsize=(10,7))\n",
    "#plt.axis(\"off\")\n",
    "ax=plt.imshow(cv2.cvtColor(crystal_edges_hand, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the crystals are labelled, compare their color to a chart to determine the path length difference\n",
    "### Calculation of the various angles is then trivial\n",
    "\n",
    "# The birefringence causes destructive interference of wavelengths in white light, yielding Newton's Color Scale\n",
    "\n",
    "### Use the Michel-Levy chart (1884, 1909, 1930, ...) of this color series to match the color of the crystal with the birefringent wavelength shift\n",
    "- Source of this chart:   https://www.geological-digressions.com/optical-mineralogy-some-terminology/\n",
    "\n",
    "# Birefringence interference color chart from Michel-Levy\n",
    "\n",
    "https://www.geological-digressions.com/optical-mineralogy-some-terminology/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlchart=plt.imread(\"../project/birefringence_chart01_.jpg\")\n",
    "plt.figure(figsize=(20,50))\n",
    "plt.imshow(mlchart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_scale=plt.imread(\"../project/thick_sample01.jpg\")\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(sample_scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure\n",
    "\n",
    "### Start by lining the images up\n",
    "- The rotation angle is noted in the original data collection notebook and is not always accurate.\n",
    "- Rotation alone (e.g. PIL rotate or hand rotation in photo editor) does not sufficiently pre-align the images, and translation to the approximate correct alignment is additionally needed.\n",
    "- Aligning images is called REGISTRATION\n",
    "- DRMIME-2D image registration works on simple images with few objects\n",
    "- Deep learning registration is not good at rotating images!\n",
    "- The best registration result required first rotating and aligning by hand with an editor, i.e. Irfanview\n",
    "\n",
    "\n",
    "### Once images are registered, identify the same crystals in each image\n",
    "- This is called SEGMENTATION and first requires MASKING of Regions Of Interest (ROI)\n",
    "- One mask for ALL images!\n",
    "\n",
    "### Given segments, find average color in each crystal/segment in each image and look up the path length\n",
    "- This requires mapping the RGB to a wavelength which is an average over the distribution of wavelengths that physically combine to produce the perceived color. It is a highly nonlinear function\n",
    "- The intereference colors are non-chromatic\n",
    "- The Michel-Levy chart and the photographs are not calibrated\n",
    "- The best possible method for this project is to find the nearest RGB in the Michel-Levy chart for each crystal / segment\n",
    "\n",
    "### Interpolate where necessary to find the black or brightest angle if azimuth is desired\n",
    "### Use the max path difference wavelength found in surveying all the images to derive the sample thickness\n",
    "### Use the sample thickness to calculate the polar angle of each crystal's C-axis\n",
    "### Plot the 3D histogram of vertical angle and summarize the C-axis angle statistically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate the images\n",
    "- approximate angle to rotate was measured in the experiment by projecting the image on the wall and tracing the edge of the slide, then measuring the trace relative to the trace of the first slide\n",
    "- Rotation of each image must be extremely precise for the unsupervised affine registration to be able to align the images\n",
    "- Registration is improved a lot by also translating each image by hand to align each as well as possible before attempting unsupervised registration\n",
    "- Conceivably, one could use ML to discover keypoints in image pairs, pick two matching keypoints in each image, calculate a line between them in each image, and calculate the rotation angle and translation required from those keypoints.\n",
    "- see ../data/SnowProject/Book1.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#angles = [0.,-16.,-20.,-29.2,-38.5,-51.,-62.5,-74.,-84.5]# L7 recorded 1993\n",
    "angles = [0.,-16.,-26.,-29,-38.5,-60.,-80,]# L7 Irfanview 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_im_list = []\n",
    "i=0\n",
    "for i in range(0,len(angles)):#im in im_list:\n",
    "    rot_im_list.append(im_list[i].rotate(angles[i],resample=0, expand=0, center=None, translate = None, fillcolor=None))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nimages=len(rot_im_list)\n",
    "#fig, ax = plt.subplots(1,nimages, figsize=(15,15));\n",
    "#for i in range(nimages):\n",
    "#    ax[i].imshow(rot_im_list[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "backimg=rot_im_list[0]\n",
    "ax.imshow(im_list[0])\n",
    "img=plt.imshow(backimg)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    backimg=rot_im_list[i]\n",
    "    imgnew=img.set_data(backimg)\n",
    "\n",
    " \n",
    "    ax.imshow(backimg)\n",
    "    ax.set_title(angles[i])\n",
    "\n",
    "    return(img)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the rotated images as TIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try:\n",
    "#    os.makedirs(datapath / archive / experiment / \"auto_rotated\")\n",
    "#except FileExistsError:\n",
    "#    # directory already exists\n",
    "#    pass\n",
    "#for i in range(len(rot_im_list)):\n",
    "#    rot_im_list[i].save((datapath / archive / experiment / \"auto_rotated\" / f\"{i}.TIFF\"), \"TIFF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The angles for 2023 were found by hand with Irfanview (GUI).\n",
    "- It does not cost much more time with the GUI to also translate the images (crop)\n",
    "- These pre-processed images were registered very well\n",
    "- Previous experiments with registration of the auto-rotated images did not work well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the images to image 0 with DRMIME_2D_ZeroCostDL4Mic.ipynb\n",
    "- I used it on colab\n",
    "- Lucas von Chamier*, Romain F. Laine*, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hernández-pérez, Pieta Mattila, Eleni Karinou, Séamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nature Communications, 2021. DOI: https://doi.org/10.1038/s41467-021-22518-0\n",
    "- load the fixed image to data/FixedImage\n",
    "- load the rotating images to data/MovingImage\n",
    "- download the result from data/Prediction\n",
    "- crop the images to an area of interest\n",
    "- Most recent successful output is from hand-rotation and translation:\n",
    "C:\\Users\\jkhac\\Documents\\AML\\Module 6 Computer Vision\\data\\SnowProject\\all\\L7\\cropped\\run3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
    "local_folder = \"../\"\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/guiwitz/MLCV/main/notebooks/check_colab.py\", \"check_colab.py\")\n",
    "from check_colab import set_datapath\n",
    "colab, datapath = set_datapath(local_folder)\n",
    "archive = \"data/SnowProject/all/\"\n",
    "experiment = \"L7\"\n",
    "basepath = datapath / archive / experiment\n",
    "print(basepath)\n",
    "imgpath = basepath / \"cropped/run3_optimized_cellpose_WB\"\n",
    "print(imgpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "im_list =[] # list of raw data images\n",
    "\n",
    "# iterate through files and add to list of images\n",
    "for file in os.listdir(imgpath):\n",
    "    if file.endswith(\"jpg\"):\n",
    "        file_path=imgpath / file\n",
    "        print(file_path)\n",
    "    #load_image(file_path)\n",
    "        im_list.append(cv2.imread(str(file_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [0.,-16.,-29,-38.5,-60.,-80,]# L7 Irfanview 2023 -- image 2 didn't register well and is omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "backimg=cv2.cvtColor(im_list[0], cv2.COLOR_BGR2RGB)\n",
    "ax.imshow(backimg)\n",
    "img=plt.imshow(backimg)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    backimg=cv2.cvtColor(im_list[i], cv2.COLOR_BGR2RGB)\n",
    "    imgnew=img.set_data(backimg)\n",
    "\n",
    " \n",
    "    ax.imshow(backimg)\n",
    "    ax.set_title(-1*angles[i])\n",
    "\n",
    "    return(img)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=plt.imshow(cv2.cvtColor(crystal_edges_hand, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance segmentation with pytorchvision\n",
    "- https://pytorch.org/vision/stable/models.html\n",
    "- https://arxiv.org/abs/1506.01497\n",
    "\n",
    "I'm not sure what this is supposed to do. The following code opens an Irfanview window with the input image and the python code keeps running without printing \"Finished\". It finishes with a warning that no boxes were drawn. Is it trained to COCO labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the Holistically Nested Edge Detection (HED) algorithm to make an edge-based mask for identifying and tracking individual crystals through the pre-registered images\n",
    "\n",
    "Main reference: https://youtu.be/un7QvhXZ_G4\n",
    "\n",
    "Original HED papr: https://arxiv.org/pdf/1504.06375.pdf\n",
    "\n",
    "\n",
    "Saining Xie, Zhuowen Tu,Holistically-Nested Edge Detection, Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015, pp. 1395-1403 \n",
    "\n",
    "\n",
    "Caffe model is encoded into two files\n",
    "1. Proto text file: https://github.com/s9xie/hed/blob/master/examples/hed/deploy.prototxt\n",
    "2. Pretrained caffe model: http://vcl.ucsd.edu/hed/hed_pretrained_bsds.caffemodel\n",
    "(information current as of October 2022)\n",
    "\n",
    "Steps for edge detection followed by connected components-based labeling\n",
    "for object segmentation:\n",
    "    \n",
    "1. Define the crop layer (not implemented by default) ​\n",
    "2. Define the network and load the pre-trained model.​\n",
    "3. Register the crop layer with the network\n",
    "4. Create blob from the image – basically create a preprocessed image​\n",
    "5. Load pretrained model (you need both the proto text and caffe model files)​\n",
    "6. Pass the blob image through model​ (forward pass)\n",
    "7. Get output​\n",
    "8. Get image ready for connected components (blur, threshold)​\n",
    "9. Perform connected components based labeling​\n",
    "10. (Optional) draw markers for visualization purposes​\n",
    "11. (Optional) filter out small objects​\n",
    "12. Export your data​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#from matplotlib import pyplot as plt\n",
    "#import numpy as np\n",
    "import imageio.v3 as iio\n",
    "from PIL import Image\n",
    "\n",
    "#import os\n",
    "\n",
    "#from matplotlib import animation\n",
    "#plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above uses JavaScript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a Crop layer that the HED network uses which is not implemented by default. To ensure that the crop is the same for each image, we need to provide our own implementation of this layer.\n",
    "- Without the crop layer, the final result will be shifted to the right and bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropLayer(object):\n",
    "    def __init__(self, params, blobs):\n",
    "        # initialize our starting and ending (x, y)-coordinates of\n",
    "        # the crop\n",
    "        self.startX = 0\n",
    "        self.startY = 0\n",
    "        self.endX = 0\n",
    "        self.endY = 0\n",
    "\n",
    "    def getMemoryShapes(self, inputs):\n",
    "        # the crop layer will receive two inputs -- we need to crop\n",
    "        # the first input blob to match the shape of the second one,\n",
    "        # keeping the batch size and number of channels\n",
    "        (inputShape, targetShape) = (inputs[0], inputs[1])\n",
    "        (batchSize, numChannels) = (inputShape[0], inputShape[1])\n",
    "        (H, W) = (targetShape[2], targetShape[3])\n",
    "\n",
    "        # compute the starting and ending crop coordinates\n",
    "        self.startX = int((inputShape[3] - targetShape[3]) / 2)\n",
    "        self.startY = int((inputShape[2] - targetShape[2]) / 2)\n",
    "        self.endX = self.startX + W\n",
    "        self.endY = self.startY + H\n",
    "\n",
    "        # return the shape of the volume (we\"ll perform the actual\n",
    "        # crop during the forward pass\n",
    "        return [[batchSize, numChannels, H, W]]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # use the derived (x, y)-coordinates to perform the crop\n",
    "        return [inputs[0][:, :, self.startY:self.endY,\n",
    "                self.startX:self.endX]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pre-trained model that OpenCV uses has been trained in the Caffe framework\n",
    "- Download from the link above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protoPath = \"C:/Users/jkhac/Documents/AML/Module 6 Computer Vision/notebooks/hed_model/deploy.prototxt\"\n",
    "modelPath = \"C:/Users/jkhac/Documents/AML/Module 6 Computer Vision/notebooks/hed_model/hed_pretrained_bsds.caffemodel\"\n",
    "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "#net = cv2.dnn.readNetFromCaffe(modelPath, protoPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the crop layer to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.dnn_registerLayer(\"Crop\", CropLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the paths to the input and output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
    "#local_folder = \"../\"\n",
    "#import urllib.request\n",
    "#urllib.request.urlretrieve(\"https://raw.githubusercontent.com/guiwitz/MLCV/main/notebooks/check_colab.py\", \"check_colab.py\")\n",
    "#from check_colab import set_datapath\n",
    "#colab, datapath = set_datapath(local_folder)\n",
    "#archive = \"data/SnowProject/all/\"\n",
    "#experiment = \"L7\"\n",
    "#basepath = datapath / archive / experiment\n",
    "#print(basepath)\n",
    "#imgpath = basepath / \"cropped/run3_optimized_cellpose_WB\"\n",
    "#print(imgpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the input image and grab its dimensions to define the blob\n",
    "- OpenCV can handle a batch of images in a list\n",
    "- OpenCV stores images as Blue, Green, Red instead of Red, Green, Blue\n",
    "- OpenCV also switches X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in a list of images for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#im_list =[] # list of raw data images\n",
    "\n",
    "# iterate through files and add to list of images\n",
    "#for file in os.listdir(imgpath):\n",
    "#    if file.endswith(\"jpg\"):\n",
    "#        file_path=imgpath / file\n",
    "#        print(file_path)\n",
    "    #load_image(file_path)\n",
    "#        im_list.append(cv2.imread(str(file_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0=im_list[0]\n",
    "img0_rgb = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\n",
    "#plt.imshow(img0_rgb)\n",
    "(H, W) = img0.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [0.,-16.,-29,-38.5,-60.,-80,]# L7 Irfanview 2023 -- image 2 didn't register well and is omitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment and label each crystal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process: adjust colors\n",
    "- gamma shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_shift(img,gamma):\n",
    "\n",
    "#create lookup table\n",
    "    values = np.arange(0, 256)\n",
    "    lut = np.uint8(255 * np.power((values/255.0), gamma))\n",
    "\n",
    "#gamma adjustment. convert image using LUT table. It maps the pixel intensities in the input to the output using values from lut\n",
    "    return cv2.LUT(img, lut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline\n",
    "- Positive gamma lets the HED algorithm find edges better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_im_list=[]\n",
    "for image in im_list:\n",
    "    processed_im_list.append(gamma_shift(image,2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(15,15))\n",
    "ax[0].imshow(cv2.cvtColor(im_list[0], cv2.COLOR_BGR2RGB))\n",
    "ax[1].imshow(cv2.cvtColor(processed_im_list[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a blob out of the input images for the DNN: consistent with images used for training the model\n",
    "- \"blob\" is a preprocessed image. \n",
    "- OpenCV’s deep neural network (dnn ) module contains two functions that can be used for preprocessing images and preparing them for classification in pre-trained deep learning models.\n",
    "- It includes scaling and mean subtraction\n",
    "- The parameters to adjust to tune the edge search are \"scale factor (sf)\" and the \"mean\" pixel intensities\n",
    " -- Higher scale factors give smaller/more segments and a log-log distributed intensity; lower scale factors give bigger/fewer segments and Normal-distributed intensities\n",
    "- https://pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_pixel_values= np.average(img0, axis = (0,1))\n",
    "mean_pixel_values=(np.average(processed_im_list, axis=(0,1,2)))\n",
    "sf = .5\n",
    "print(f\"Mean pixel values {mean_pixel_values} and scale factor {sf}\")\n",
    "blob = cv2.dnn.blobFromImages(processed_im_list, scalefactor=sf, size=(W, H),\n",
    "                             mean=(mean_pixel_values[0], mean_pixel_values[1], mean_pixel_values[2]),\n",
    "                             #mean=(178, 211, 173),\n",
    "                             #mean = (150,190,150),\n",
    "                             swapRB= False, crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#garbage=plt.hist(blob[0,1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View image after preprocessing (blob batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages=blob.shape[0]\n",
    "fig, ax = plt.subplots(1,nimages, figsize=(25,25));\n",
    "for i in range(nimages):\n",
    "    blob_for_plot = np.moveaxis(blob[i,:,:,:], 0,2)\n",
    "    ax[i].imshow(blob_for_plot)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the blob batch as the input to the holistically nested edge detection (HED) network and perform a forward pass to compute the edges of each image\n",
    "- Result is a single-channel float32 image for each image in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.setInput(blob)\n",
    "hed = net.forward()\n",
    "#hed = hed[0,0,:,:]  #Drop the other axes \n",
    "#hed = cv2.resize(hed[0, 0], (W, H))\n",
    "#hed = (255 * hed).astype(\"uint8\")  #rescale to 0-255\n",
    "hed = (255 * hed)  #rescale to 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hed.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the edge masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimages=hed.shape[0]\n",
    "fig, ax = plt.subplots(1,nimages, figsize=(25,25));\n",
    "for i in range(nimages):\n",
    "    hed_for_plot = np.moveaxis(hed[i,:,:,:], 0,2)\n",
    "    ax[i].imshow(hed_for_plot)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "backimg=np.moveaxis(hed[0,:,:,:], 0,2)\n",
    "ax.imshow(backimg)\n",
    "img=plt.imshow(backimg)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    backimg=np.moveaxis(hed[i,:,:,:], 0,2)\n",
    "    imgnew=img.set_data(backimg)\n",
    "\n",
    " \n",
    "    ax.imshow(backimg)\n",
    "    ax.set_title(-1*angles[i])\n",
    "\n",
    "    return(img)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since some crystals are indistinguishable from background in some images, consolidate the batch edge masks to capture all the edges in one mask by summing the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedsum=hed[0,0,:,:]*0.\n",
    "for i in range(hed.shape[0]):\n",
    "    hedsum=np.add(hedsum, hed[i,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedsum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale (average) the edge mask values and make them integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hedsum=(hedsum/hed.shape[0]).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hedsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear scaling to highlight peaks against background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(hedsum**3.)\n",
    "test=hedsum.copy()\n",
    "test=np.clip((test**1.2),0,255)\n",
    "test[test<100]=0\n",
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of pixel intensities in edge mask and in nonlinear scaled edge mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten image and plot histogram\n",
    "image3_flat=(hedsum**3.).flatten()\n",
    "ax=plt.hist(image3_flat,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of pixel intensities in edge mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten image and plot histogram\n",
    "image_flat=(hedsum).flatten()\n",
    "ax=plt.hist(image_flat,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the pixels in the 100 x 100 image green channel that are less bright than the x-th percentile\n",
    "test=hedsum**3.\n",
    "test=test/test.max()\n",
    "#threshold=np.quantile(test,threshold_val)\n",
    "threshold = .05# test is scaled 0-1\n",
    "mask1=test < threshold\n",
    "mask2=test >= threshold\n",
    "#print(mask1.shape)\n",
    "pix_u_threshold=test.copy()\n",
    "pix_u_threshold[mask1]=255\n",
    "pix_u_threshold[mask2]=0\n",
    "pix_u_threshold=pix_u_threshold.astype(\"uint8\")\n",
    "#print(pix_u_threshold.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold image to accentuate bright pixels -- compare with threshold filters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pix_u_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Connected component based labeling\n",
    "\n",
    "- Load segmented binary image, Gaussian blur, grayscale, Otsu\"s threshold\n",
    "- Note that Otsu\"s threshold is hardly different from slashing out the low and high as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blur = cv2.GaussianBlur(hedsum, (5,5), 0)#hedsum\n",
    "blur = cv2.bilateralFilter(hedsum,9,5,10)\n",
    "thresh0 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "#thresh0 = cv2.threshold(blur, 0, 255, cv2.THRESH_OTSU)[1]\n",
    "#plt.imshow(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(thresh0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low=127\n",
    "high=255\n",
    "ret,thresh1 = cv2.threshold(blur,low,high,cv2.THRESH_BINARY)\n",
    "ret,thresh2 = cv2.threshold(blur,low,high,cv2.THRESH_BINARY_INV)\n",
    "ret,thresh3 = cv2.threshold(blur,low,high,cv2.THRESH_TRUNC)\n",
    "#ret,thresh4 = cv2.threshold(blur,low,high,cv2.THRESH_TOZERO)\n",
    "#ret,thresh5 = cv2.threshold(blur,low,high,cv2.THRESH_TOZERO_INV)\n",
    "thresh4 = cv2.adaptiveThreshold(hedsum,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,2)\n",
    "thresh5 = cv2.adaptiveThreshold(hedsum,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,0)\n",
    "#titles = [\"Original Image\",\"BINARY\",\"BINARY_INV\",\"TRUNC\",\"TOZERO\",\"TOZERO_INV\"]\n",
    "titles = [\"Original Image\",\"BINARY\",\"BINARY_INV\",\"TRUNC\",\"ADAPT MEAN\",\"ADAPT GAUSS\"]\n",
    "images = [img0_rgb, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1),plt.imshow(images[i],\"gray\",vmin=0,vmax=255)\n",
    "    plt.title(titles[i])\n",
    "    #plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform connected component labeling\n",
    "## Use a threshold image (0 (background)and 255 (regions of interest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh=255-thresh5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mybins=(np.arange(0,256,1).tolist())\n",
    "#garbage=plt.hist(thresh,bins=mybins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(thresh), thresh.shape,thresh.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(thresh>100).sum()/(thresh.shape[0]*thresh.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(thresh, connectivity=4)#4 just X,Y or 8 with diagonals  -- changes the segmentation but not the end result (size or angle distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create false color image with black background and colored objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.random.randint(0, 255, size=(n_labels, 3), dtype=np.uint8)\n",
    "colors[0] = [0, 0, 0]  # black background\n",
    "false_colors = colors[labels]\n",
    "fig,ax=plt.subplots(1,2,figsize=(20,24))\n",
    "ax[0].imshow(false_colors)\n",
    "ax[1].imshow(crystal_edges_hand)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_labels,labels.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain centroids\n",
    "- instead of drawing centroids, draw the ID of each segment so we can investigate further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_colors_centroid = false_colors.copy() # overlay any of the images with the centroid markers, for example rgb_img\n",
    "for centroid in centroids:\n",
    "    cv2.drawMarker(false_colors_centroid, (int(centroid[0]), int(centroid[1])),\n",
    "                   color=(255, 255, 255), markerType=cv2.MARKER_CROSS)\n",
    "plt.imshow(false_colors_centroid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove objects that are too big and/or small\n",
    "- add the segment number to the overlay plot\n",
    "- remove filtered segments from the false_colors_filtered layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_AREA = 9*9\n",
    "MAX_AREA = 80*80\n",
    "false_colors_area_filtered = false_colors.copy()\n",
    "filter_index=[]\n",
    "for i, centroid in enumerate(centroids[1:], start=1):# centroid[0] is the background\n",
    "    area = stats[i, 4]\n",
    "    if area > MIN_AREA and area < MAX_AREA:\n",
    "        cv2.drawMarker(false_colors_area_filtered, (int(centroid[0]), int(centroid[1])),\n",
    "                      color=(255, 255, 255), markerType=cv2.MARKER_CROSS)\n",
    "        cv2.putText(false_colors_area_filtered, str(i), (int(centroid[0]), int(centroid[1])),\n",
    "                       fontFace=4,fontScale=1.,color=(255, 255, 255))\n",
    "    else:\n",
    "       # print(i, area)\n",
    "        filter_index.append(i) # index of which segments will be filtered out\n",
    "    \n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(221)\n",
    "plt.imshow(img0_rgb)\n",
    "plt.subplot(222)\n",
    "plt.imshow(hedsum)\n",
    "plt.subplot(223)\n",
    "plt.imshow(thresh)\n",
    "plt.subplot(224)\n",
    "plt.imshow(false_colors_area_filtered) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filter_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace labels of the filtered-out regions of interest with 0\n",
    "false_colors_filtered=false_colors.copy()\n",
    "for i in range(len(filter_index)):\n",
    "    false_colors_filtered[labels==filter_index[i]]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot isn't showing expanses of black (color[0]) as expected ?!\n",
    "plt.imshow(false_colors_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlay an alpha image of the continuous clusters on the original image0 and/or the centroid crosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.imshow(img0_rgb)\n",
    "ax.imshow(false_colors_filtered, alpha=.4)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "#fig=plt.figure()\n",
    "backimg=img0_rgb\n",
    "ax.imshow(img0_rgb)\n",
    "ax.imshow(false_colors_filtered, alpha=0.3)\n",
    "ax.set_axis_off()\n",
    "img=plt.imshow(backimg)\n",
    "#img=plt.imshow(img_rgb)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    backimg=cv2.cvtColor(im_list[i], cv2.COLOR_BGR2RGB)\n",
    "    imgnew=img.set_data(backimg)\n",
    "\n",
    " \n",
    "    ax.imshow(backimg)\n",
    "    ax.imshow(false_colors_filtered, alpha=0.3)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    return(img)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many crystals easily discerned with the eye were not captured by the edge algorithm\n",
    "## Some segments contain several crystals and should be individual segments\n",
    "## Need to check the resulting segments by hand for errors before processing further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# We can use regionprops from skimage.measures to extract various parameters\n",
    "\n",
    "- module calculates useful parameters for each object in one or a batch of images.\n",
    "\n",
    "- first, define the function to convert RGB to path difference (wavelength in nm)\n",
    "\n",
    "- make a props object for each image in image list\n",
    "- add features: azimuth angle, path difference wavelength\n",
    "- drop superfluous variables\n",
    "- add the coordinates (centroid x,y) to the dataframe --> easiest to do this before filtering\n",
    "- concatenate and make pandas df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Michel-Levy chart (1884, 1930) of this color series to match the color of the crystal with the birefringent wavelength shift\n",
    "- Source of this chart:   https://www.geological-digressions.com/optical-mineralogy-some-terminology/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlchart_full = cv2.imread(str(datapath / \"project/birefringence_chart01_.jpg\"))\n",
    "fig, ax = plt.subplots(1,1,figsize=(20,100))\n",
    "plt.axis(\"off\")\n",
    "ax=plt.imshow(cv2.cvtColor(mlchart_full, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate path length wavelength to derive the birefringence and thus the polar angle\n",
    "### \"Don't trust your eyes\" -- know how your sensor and the computer are processing the light\n",
    "- https://bioimagebook.github.io/chapters/1-concepts/1-images_and_pixels/images_and_pixels.html#chap-pixels\n",
    "\n",
    " - Many RGB colors do not have wavelength (mix of wavelengths) --> dominant wavelength\n",
    " - Use the Hue as approximation. Find a RGB_to_Hue or HSV converter shorthand\n",
    " - Use the \"colour\" package\n",
    " - Sample the Michel-Levy chart until a RGB match is found. Read off the value of wavelength shift.\n",
    " - add the path difference wavelength variable to the df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Read in the michel-levy chart\n",
    "# record the BGR values at all pixels at X=[250,1800], Y[750]\n",
    "# calculate the wavelength linear with X (pixel), Lamdba (nm) => 250,0 and 1744,1755\n",
    "\n",
    "mlchart = cv2.imread(str(datapath / \"project/birefringence_chart01_copy.jpg\"))\n",
    "#mlchart=cv2.cvtColor(mlchart, cv2.COLOR_BGR2RGB)\n",
    "mlchart.shape\n",
    "\n",
    "# Note x and y are reversed from jpg!\n",
    "mlchart_shift=gamma_shift(mlchart,1.)\n",
    "ml_pixels_row=mlchart_shift[797:803,250:1800,:]\n",
    "ml_pixels_row.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look up the color of each crystal on this color chart and match it to the path difference on the X-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_img=cv2.cvtColor(mlchart_shift[797:803,250:1800,:],cv2.COLOR_BGR2RGB)\n",
    "xx=np.mean(ml_pixels_row, axis=0)\n",
    "pathdiff_wavelength = np.array(range(0,xx.shape[0]))\n",
    "pathdiff_wavelength=pathdiff_wavelength*(1744.)/(1754-250)\n",
    "xx=pathdiff_wavelength\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,100))\n",
    "ax[0].imshow(spectral_img, extent=[0, 1796, 0, 20*spectral_img.shape[0]])\n",
    "ax[0].plot(xx, 0*xx, '-', linewidth=0, color=\"black\")\n",
    "ax[0].set_xlabel(\"Path Difference (nm)\")\n",
    "ax[0].set_xlim(0,1800)\n",
    "\n",
    "\n",
    "ax[1].imshow(cv2.cvtColor(im_list[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The colors in the color bar don't seem to be the same hue/saturation as those in the data sample. Match the RGB histogram of the bar to that of the data image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imsave\n",
    "from skimage import exposure\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "# Load left and right images\n",
    "L = spectral_img\n",
    "R = cv2.cvtColor(im_list[0], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Match using the right side as reference\n",
    "matched = match_histograms(L, R, multichannel=True)\n",
    "\n",
    "# Place side-by-side and save\n",
    "#result = np.hstack((matched,R))\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,100))\n",
    "ax[0].imshow(matched, extent=[0, 1796, 0, 20*L.shape[0]])\n",
    "ax[0].plot(xx, 0*xx, '-', linewidth=0, color=\"black\")\n",
    "ax[0].set_xlabel(\"Path Difference (nm)\")\n",
    "ax[0].set_xlim(0,1800)\n",
    "\n",
    "\n",
    "ax[1].imshow(R)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That didn't look right at all. Don't use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pixels_row=np.mean(ml_pixels_row, axis=0) # this is the average of 6 pixels at each wavelength from the mlchart\n",
    "#ml_pixels_row=np.mean(matched, axis=0) # this is the average of 6 pixels at each wavelength from the histogram-adjusted mlchart\n",
    "ml_pixels_row.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathdiff_wavelength = np.array(range(0,ml_pixels_row.shape[0]))\n",
    "pathdiff_wavelength=pathdiff_wavelength*(1744.)/(1754-250)# read off the chart: calibration of wavelength and pixel number\n",
    "max_pathdiff_to_search = 589 # nm\n",
    "max_index=int(max_pathdiff_to_search*(1754-250)/1744.)\n",
    "\n",
    "# row of pixels sampled from Michel-Levy chart is in BGR\n",
    "# pixel to compare should be in RGB\n",
    "\n",
    "def rgb_to_pathdiff(rgb):\n",
    "    #rgb=[86,99,134]#664 /668\n",
    "    #rgb=[161,81,50]#430 /436\n",
    "    #rgb=[178,77,145]#1101 /1098\n",
    "    #rgb=[90,146,75]#  /1211\n",
    "    #rgb=[218,214,45] #/824\n",
    "\n",
    "    pd_lambda=[]\n",
    "    for j in range(0,rgb.shape[0]):\n",
    "        dist=1000000\n",
    "# For all wavelengths        \n",
    "#        for i in range(0,ml_pixels_row.shape[0]):\n",
    "# For only the first order wavelengths (to ca. 920 nm)\n",
    "        for i in range(0,max_index):\n",
    "            newdist=(ml_pixels_row[i,0]-rgb[j,2])**2+(ml_pixels_row[i,1]-rgb[j,1])**2+(ml_pixels_row[i,2]-rgb[j,0])**2\n",
    "            \n",
    "            if(newdist<dist):\n",
    "                dist=newdist\n",
    "                mini=i\n",
    "                #print(f\"j {j} dist {dist} i {i} wl {pathdiff_wavelength[i]} img rgb {rgb[j,0]} {rgb[j,1]} {rgb[j,2]}\")\n",
    "        pd_lambda.append(pathdiff_wavelength[mini])\n",
    "        #print(f\"Wavelength at pixel {rgb[j,:]} for crystal {j} is {pathdiff_wavelength[mini]:.0f} nm\")\n",
    "    return(np.asarray(pd_lambda))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the lookup function in the Michel-Levy table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the lookup function in the Michel-Levy table\n",
    "myrgb=np.zeros((2,3))\n",
    "myrgb[0,:]=[161,81,50]#430 /436\n",
    "myrgb[1,:]=[143,56,73] # /520\n",
    "\n",
    "print(rgb_to_pathdiff(myrgb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the pixel intensity vs. path difference of the unmodified color bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful, the cv2 format is yx and bgr, not xy and rgb\n",
    "\n",
    "plt.plot(pathdiff_wavelength, ml_pixels_row[:,0],color=\"blue\")\n",
    "plt.plot(pathdiff_wavelength, ml_pixels_row[:,1],color=\"green\")\n",
    "plt.plot(pathdiff_wavelength, ml_pixels_row[:,2],color=\"red\")\n",
    "plt.plot(pathdiff_wavelength, ml_pixels_row[:,0]+ml_pixels_row[:,1]+ml_pixels_row[:,2], color=\"gray\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the pixel intensity vs. path difference of the histogram-matched color bar chart: chaotic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful, the cv2 format is yx and bgr, not xy and rgb\n",
    "\n",
    "plt.plot(pathdiff_wavelength, matched[1,:,0],color=\"blue\")\n",
    "plt.plot(pathdiff_wavelength, matched[1,:,1],color=\"green\")\n",
    "plt.plot(pathdiff_wavelength, matched[1,:,2],color=\"red\")\n",
    "plt.plot(pathdiff_wavelength, matched[1,:,0]+matched[1,:,1]+matched[1,:,2], color=\"gray\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the histogram of the histogram-matched color bar -- noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ('b','g','r')\n",
    " \n",
    "#Loop through each color sequentially\n",
    "for i,col in enumerate(color):\n",
    "     \n",
    "    #To use OpenCV's calcHist function, uncomment below\n",
    "    #histr = cv2.calcHist([colorimage],[i],None,[256],[0,256])\n",
    "     \n",
    "    #To use numpy histogram function, uncomment below\n",
    "    testimg=mlchart[797:803,250:1800,:]\n",
    "    testimg=mlchart_shift[797:803,250:1800,:]\n",
    "    testimg=matched\n",
    "    testimg2=im_list[5]\n",
    "    histr1, _ = np.histogram(testimg[:,:,i],256,[0,256],density=True)\n",
    "    histr2, _ = np.histogram(testimg2[:,:,i],256,[0,256], density=True)\n",
    "    plt.plot(histr1,color = col)\n",
    "    plt.plot(histr2,color = col, linestyle=\"dashed\")\n",
    "    plt.xlim([0,256])\n",
    "    plt.ylim(0,0.070)\n",
    "    #fig,ax=plt.subplots(1,2,figsize=(8,8))\n",
    "    #ax[0].plot(histr1,color = col)  #lookup chart\n",
    "    #ax[1].plot(histr2,color = col)  #data image\n",
    "    \n",
    "     \n",
    "plt.show()  #Show our plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the matching of the crystal color to the color in the chart by difference in R,G,B value\n",
    "### Maybe using normalized intensities would approximate matching the hue of the images?\n",
    "### Dot-product of the color vectors gave very unexpected results: no input test values returned the correct wavelength shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "import pandas as pd\n",
    "\n",
    "props_batch=measure.regionprops_table(labels, intensity_image=im_list[0], properties=[\"label\",\n",
    "                                          \"area\", \"equivalent_diameter\",\n",
    "                                          \"mean_intensity\", \"solidity\"])\n",
    "\n",
    "rgb=np.zeros((len(props_batch[\"label\"]),3))\n",
    "# In object rgb: red 0, green 1, blue 2\n",
    "# In df red is 2, green 1, blue 0\n",
    "rgb[:,0]=props_batch[\"mean_intensity-2\"]\n",
    "rgb[:,1]=props_batch[\"mean_intensity-1\"]\n",
    "rgb[:,2]=props_batch[\"mean_intensity-0\"]\n",
    "props_batch[\"pathdiff0\"]=rgb_to_pathdiff(rgb)\n",
    "props_batch[\"az0\"]=angles[0]\n",
    "props_batch[\"centroid_X\"]=centroids[1:,0]\n",
    "props_batch[\"centroid_Y\"]=centroids[1:,1]\n",
    "\n",
    "del props_batch[\"mean_intensity-0\"]\n",
    "del props_batch[\"mean_intensity-1\"]\n",
    "del props_batch[\"mean_intensity-2\"]\n",
    "\n",
    "# Make a dataframe for later manipulation\n",
    "df = pd.DataFrame(props_batch)\n",
    "\n",
    "for i in range(1,len(im_list)):\n",
    "    props_img=measure.regionprops_table(labels,intensity_image=im_list[i], properties=[\"label\", \"mean_intensity\"])\n",
    "    \n",
    "    rgb[:,0]=props_img[\"mean_intensity-2\"]\n",
    "    rgb[:,1]=props_img[\"mean_intensity-1\"]\n",
    "    rgb[:,2]=props_img[\"mean_intensity-0\"]\n",
    "    props_img[\"pathdiff\"+str(i)]=rgb_to_pathdiff(rgb)\n",
    "    props_img[\"az\"+str(i)]=-1*angles[i]\n",
    "    \n",
    "    del props_img[\"mean_intensity-0\"]\n",
    "    del props_img[\"mean_intensity-1\"]\n",
    "    del props_img[\"mean_intensity-2\"]\n",
    "    del props_img[\"label\"]\n",
    "\n",
    "    df_img=pd.DataFrame(props_img)\n",
    "\n",
    "# concatenate props to props_batch df with key \"label\"\n",
    "    df=pd.concat([df, df_img,], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the segments that are too small or too large or otherwise marked for filtration\n",
    "print(filter_index), len(filter_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must subtract one from filter_index to account for zero centroid which is the background (added to the df)\n",
    "filter_index_minus_one=[x-1 for x in filter_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check: YES the measures.regioprop function displaces the ROI index by one because it adds the region \"0\" for the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[filter_index_minus_one[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(filter_index_minus_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove the segments corresponding to the mask above (filter_index)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the maximum path difference of each crystal and add this variable to the df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathdifflist=[]\n",
    "for i in range(1,len(im_list)):\n",
    "    pathdifflist.append(\"pathdiff\"+str(i))\n",
    "print(pathdifflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df doesn't accept the list of column names generated above so cut and paste the output of the previous cell here:\n",
    "df[\"path_diff\"]=df[['pathdiff1', 'pathdiff2', 'pathdiff3', 'pathdiff4', 'pathdiff5']].max(axis=1) # new variable is maximum path difference --> calcualte polar angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the angle of the darkest appearance of each crystal and add this variable to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"extinction_az\"]=df[[pathdifflist]].min(axis=1) # new variable is azimuth at minimum path difference --> roughly indicates azimuth angle (interpolation necessary=\n",
    "# use .loc to get the index of the column of min and to select corresponding azimuth column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a new reduced df with only label, shape, etc.,  wavelength and corresponding angle of brightest and darkest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate polar angle and add to reduced df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne\t= 1.310893\n",
    "sample_temp = -16.0 # deg C\n",
    "nw =1.309-(11.4+3.84*sample_temp)*0.00001\n",
    "largest_sample_path_diff = df[\"path_diff\"].max() # nm\n",
    "sample_thickness = (largest_sample_path_diff / (ne - nw))*0.000000001*1000 # mm\n",
    "df[\"ne_prime\"]= (df[\"path_diff\"]/sample_thickness)*0.000001 + nw\n",
    "df[\"polar_angle\"] = np.arcsin((((nw/df[\"ne_prime\"])**2.-1.)/((nw/ne)**2.-1.))**0.5)*180./np.pi\n",
    "print(f\"max_pd {largest_sample_path_diff:.0f}, thickness {sample_thickness:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df[\"path_diff\"] == largest_sample_path_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot polar angle, maybe azimuth, report average and std dev\n",
    "## Expect a random distribution of polar angles to correspond to the directions on the surface of a sphere: the average angle is the center of mass of the spherical shell at 60 degrees from vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage=plt.hist(df[\"area\"], bins=20)\n",
    "print(\"Average Area and StdDev\")\n",
    "print(df[\"area\"].mean(), np.std(df[\"area\"]))\n",
    "print(\"Number of valid crystals\")\n",
    "print(len(df[\"polar_angle\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the axes\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax  = fig.add_subplot(111)\n",
    "# now plot\n",
    "data=df[\"polar_angle\"]\n",
    "myHist = ax.hist(data, 20)\n",
    "avg=np.linspace(0,int(myHist[0].max()))\n",
    "x = np.zeros(50)+60.\n",
    "h = ax.plot(x, avg, lw=2)\n",
    "# show\n",
    "plt.show()\n",
    "print(\"Average Polar Angle and StdDev\")\n",
    "print(df[\"polar_angle\"].mean(), np.std(df[\"polar_angle\"]))\n",
    "print(\"Number of valid crystals\")\n",
    "print(len(df[\"polar_angle\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myHist[0].max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap in path difference from 500-610 and 770-830 circa and correspondingly no angles in this range, specifically 50-60 degrees\n",
    "- This is where blue is highest intensity in the color chart, red is low, green is medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[\"path_diff\"],df[\"polar_angle\"],marker=\".\", linestyle=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "#from matplotlib import pyplot as plt\n",
    "#import numpy as np\n",
    "#import imageio.v3 as iio\n",
    "#from PIL import Image\n",
    "\n",
    "#import os\n",
    "\n",
    "#from matplotlib import animation\n",
    "#plt.rcParams[\"animation.html\"] = \"jshtml\"  # for matplotlib 2.1 and above uses JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path containing data folder or use default for Colab (/gdrive/My Drive)\n",
    "#local_folder = \"../\"\n",
    "#import urllib.request\n",
    "#urllib.request.urlretrieve(\"https://raw.githubusercontent.com/guiwitz/MLCV/main/notebooks/check_colab.py\", \"check_colab.py\")\n",
    "#from check_colab import set_datapath\n",
    "#colab, datapath = set_datapath(local_folder)\n",
    "#archive = \"data/SnowProject/all/\"\n",
    "#experiment = \"L7\"\n",
    "#basepath = datapath / archive / experiment\n",
    "print(basepath)\n",
    "#imgpath = basepath / \"cropped/run3_optimized_cellpose_WB\"\n",
    "print(imgpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pachitariu, M. & Stringer, C. (2022). Cellpose 2.0: how to train your own model. Nature methods.\n",
    "\n",
    "conda activate cellpose\n",
    "\n",
    "python -m cellpose\n",
    "\n",
    "model results under run3 used the cyto model with the calculated average cell diameter of 28.2 pixels (default pre-training value is 30) and the segmentation channel 1 gray, channel 2 none -- all default values\n",
    "\n",
    "without training a new model, use cyto model with flow_threshold higher than default of 0.4 and cellprob_threshold lower than default of 0.0. For example ft=0.8 to 0.95 (up to 3) and cp= -4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "from cellpose import plot, utils\n",
    "dat = np.load(str(imgpath) + \"/4c_seg.npy\", allow_pickle=True).item()\n",
    "\n",
    "# plot image with masks overlaid\n",
    "mask_RGB = plot.mask_overlay(dat['img'], dat['masks'],\n",
    "                        colors=np.array(dat['colors']))\n",
    "\n",
    "# plot image with outlines overlaid in red\n",
    "outlines = utils.outlines_list(dat['masks'])\n",
    "plt.imshow(dat['img'])\n",
    "for o in outlines:\n",
    "    plt.plot(o[:,0], o[:,1], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[\"outlines\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each mask and each outline object contain integers indicating the correspondence of a pixel to the ID of a segment\n",
    "## Each image has been processed independently and was divided into segments differently from the other images\n",
    "## Thus there is a different number of segments for each image and each crystal in each data image has a different ID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dat[\"masks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the mask layers (here, \"outlines\") together to make one composite mask layer\n",
    "### This trick does not work if we use the segmentation directly -- only if we use the outlines in a subsequent connected-component analysis\n",
    "### The connected-component analysis based on this summed outline mask did not work well (very few components were identified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "seg_npy_list =[] # list of raw data images\n",
    "\n",
    "# iterate through files and add to list of images\n",
    "for file in os.listdir(imgpath):\n",
    "    if file.endswith(\"npy\"):\n",
    "        file_path=imgpath / file\n",
    "    #    print(file_path)\n",
    "    #load_image(file_path)\n",
    "        seg_npy_list.append(np.load(file_path, allow_pickle=True).item())\n",
    "\n",
    "#dat = np.load(str(imgpath) + \"/0c_seg.npy\", allow_pickle=True).item()\n",
    "\n",
    "# plot image with masks overlaid\n",
    "edges=seg_npy_list[0][\"outlines\"]\n",
    "#mask=seg_npy_list[0][\"masks\"]\n",
    "#labels=seg_npy_list[0][\"masks\"]\n",
    "for i in range(0,len(seg_npy_list)):\n",
    "    edges=edges+seg_npy_list[i][\"outlines\"]\n",
    "    print(i, seg_npy_list[i][\"filename\"],seg_npy_list[i][\"outlines\"].max())\n",
    "    #print(f\"file seg_npy_list{i} max segments {(seg_npy_list[i][\"outlines\"]).max()}\")\n",
    "    #mask=mask+seg_npy_list[i][\"masks\"]\n",
    "\n",
    "edges=np.clip(edges,0,1)\n",
    "#mask=np.clip(mask,0,1)\n",
    "#mask_RGB = plot.mask_overlay(dat['img'], dat['masks'],\n",
    "#                        colors=np.array(dat['colors']))\n",
    "\n",
    "# plot image with outlines overlaid in red\n",
    "\n",
    "#outlines = utils.outlines_list(dat['masks'])\n",
    "#plt.imshow(dat['img'])\n",
    "#for o in outlines:\n",
    "#    plt.plot(o[:,0], o[:,1], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "backimg=seg_npy_list[0][\"outlines\"]\n",
    "ax.imshow(backimg)\n",
    "img=plt.imshow(backimg)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    backimg=seg_npy_list[i][\"outlines\"]\n",
    "    imgnew=img.set_data(backimg)\n",
    "\n",
    "    ax.imshow(backimg)\n",
    "    ax.set_title(i)\n",
    "\n",
    "    return(img)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crystal_edges_hand = cv2.imread(str(datapath / \"project/L7_1993_crop.jpg\"))\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,24))\n",
    "ax[0].imshow(edges)\n",
    "ax[1].imshow(crystal_edges_hand)\n",
    "\n",
    "#plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking a crystal across images\n",
    "\n",
    "- There is a EPFL program in Java called \"TrackMate\" (ZeroCostDL4Mic) that tracks cells segmented in CellPose as they move in image to image\n",
    "\n",
    "- Converting the masks and outlines from CellPose _seg.npy object to something sckikit can use in .measure : save mask layer (\"outlines\" in CellPose jargon) as np array or jpg. Read into \"EdgeDetectSegmentation\" and use in place of hedsum edge image. The rest of teh processing is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue analysis as with HED. As a test, use the edge mask from the first image to generate labels with connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = seg_npy_list[0]['outlines']\n",
    "mask=edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask.max(), mask.min(), mask.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenCV connected component module the mask must be maximum 255 (feature or segment) and minimum 0 (background)\n",
    "#thresh=(255*mask/mask.max()).astype(\"uint8\")\n",
    "thresh=np.clip(mask,0,1)\n",
    "thresh=(thresh*255).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh=255-thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = cv2.bilateralFilter(thresh,9,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh5 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(thresh), thresh.shape,(thresh<100).sum()/(thresh.shape[0]*thresh.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(thresh, connectivity=4)#4 or 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV generates too many connected components based on the additive \"outlines\" object from cellpose\n",
    "# Improvement would require smoothing of boundaries, blurring\n",
    "# It is better to use the cellpose components in the \"masks\" object to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.random.randint(0, 255, size=(n_labels, 3), dtype=np.uint8)\n",
    "colors[0] = [0, 0, 0]  # black background\n",
    "false_colors = colors[labels]\n",
    "fig,ax=plt.subplots(1,2,figsize=(20,24))\n",
    "ax[0].imshow(false_colors)\n",
    "ax[1].imshow(crystal_edges_hand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create false color image with black background and colored objects\n",
    "## DO NOT USE the OpenCV connected components above; use the CellPose mask instead\n",
    "## Since each cellpose mask is different for each image, use the \"best\" one for all images\n",
    "## The \"masks\" object is used to define \"labels\" which group pixels in a data frame of the regions of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=seg_npy_list[1][\"masks\"]\n",
    "n_labels=labels.max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_labels,labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.random.randint(0, 255, size=(n_labels, 3), dtype=np.uint8)\n",
    "colors[0] = [0, 0, 0]  # black background\n",
    "false_colors = colors[labels]\n",
    "fig,ax=plt.subplots(1,2,figsize=(20,24))\n",
    "ax[0].imshow(false_colors)\n",
    "ax[1].imshow(crystal_edges_hand)\n",
    "#plt.imshow(false_colors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the original images, extract statistics of the crystals with the mask from CellPose\n",
    "angles = [0.,-16.,-29,-38.5,-60.,-80,]# L7 Irfanview 2023 -- image 2 didn't register well and is omitted\n",
    "#im_list =[] # list of raw data images\n",
    "\n",
    "# iterate through files and add to list of images\n",
    "#for file in os.listdir(imgpath):\n",
    "#    if file.endswith(\"jpg\"):\n",
    "#        file_path=imgpath / file\n",
    "#        print(file_path)\n",
    "#    #load_image(file_path)\n",
    "#        im_list.append(cv2.imread(str(file_path)))\n",
    "#processed_im_list=im_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "#fig=plt.figure()\n",
    "backimg=processed_im_list[0]\n",
    "ax.imshow(backimg)\n",
    "ax.imshow(edges, alpha=0.5)\n",
    "ax.set_axis_off()\n",
    "img=plt.imshow(backimg)\n",
    "#img=plt.imshow(img_rgb)\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    backimg=im_list[i]\n",
    "    imgnew=img.set_data(backimg)\n",
    "\n",
    " \n",
    "    ax.imshow(backimg)\n",
    "    ax.imshow(edges, alpha=0.5)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    return(img)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the labelled segments (ROI) with an overlay over an original image\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax.imshow(processed_im_list[0])\n",
    "ax.imshow(false_colors, alpha=0.5)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# We can use regionprops from skimage to extract various parameters\n",
    "\n",
    "- module calculates useful parameters for each object in one or a batch of images.\n",
    "\n",
    "- first, define the function to convert RGB to path difference (wavelength in nm)\n",
    "\n",
    "- make a props object for each image in image list\n",
    "- add features: azimuth angle, path difference wavelength\n",
    "- drop superfluous variables\n",
    "- add the coordinates (centroid x,y) to the dataframe --> easiest to do this before filtering\n",
    "- concatenate and make pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "import pandas as pd\n",
    "\n",
    "props_batch=measure.regionprops_table(labels, intensity_image=im_list[0], properties=[\"label\",\n",
    "                                          \"area\", \"equivalent_diameter\",\n",
    "                                          \"mean_intensity\", \"solidity\"])\n",
    "\n",
    "rgb=np.zeros((len(props_batch[\"label\"]),3))\n",
    "rgb[:,0]=props_batch[\"mean_intensity-2\"]\n",
    "rgb[:,1]=props_batch[\"mean_intensity-1\"]\n",
    "rgb[:,2]=props_batch[\"mean_intensity-0\"]\n",
    "props_batch[\"pathdiff0\"]=rgb_to_pathdiff(rgb)\n",
    "props_batch[\"az0\"]=angles[0]\n",
    "#props_batch[\"centroid_X\"]=centroids[1:,0]\n",
    "#props_batch[\"centroid_Y\"]=centroids[1:,1]\n",
    "\n",
    "del props_batch[\"mean_intensity-0\"]\n",
    "del props_batch[\"mean_intensity-1\"]\n",
    "del props_batch[\"mean_intensity-2\"]\n",
    "\n",
    "# Make a dataframe for later manipulation\n",
    "df2 = pd.DataFrame(props_batch)\n",
    "\n",
    "for i in range(1,len(im_list)):\n",
    "    props_img=measure.regionprops_table(labels,intensity_image=im_list[i], properties=[\"label\", \"mean_intensity\"])\n",
    "    \n",
    "    rgb[:,0]=props_img[\"mean_intensity-2\"]\n",
    "    rgb[:,1]=props_img[\"mean_intensity-1\"]\n",
    "    rgb[:,2]=props_img[\"mean_intensity-0\"]\n",
    "    props_img[\"pathdiff\"+str(i)]=rgb_to_pathdiff(rgb)\n",
    "    props_img[\"az\"+str(i)]=-1*angles[i]\n",
    "    \n",
    "    del props_img[\"mean_intensity-0\"]\n",
    "    del props_img[\"mean_intensity-1\"]\n",
    "    del props_img[\"mean_intensity-2\"]\n",
    "    del props_img[\"label\"]\n",
    "\n",
    "    df_img=pd.DataFrame(props_img)\n",
    "\n",
    "# concatenate props to props_batch df with key \"label\"\n",
    "    df2=pd.concat([df2, df_img,], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the maximum path difference of each crystal and add this variable to the df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathdifflist=[]\n",
    "for i in range(1,len(processed_im_list)):\n",
    "    pathdifflist.append(\"pathdiff\"+str(i))\n",
    "print(pathdifflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df doesn't accept the list of column names generated above so cut and paste the output of the previous cell here:\n",
    "df2[\"path_diff\"]=df2[['pathdiff1', 'pathdiff2', 'pathdiff3', 'pathdiff4', 'pathdiff5']].max(axis=1) # new variable is maximum path difference --> calcualte polar angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the angle of the darkest image of each crystal and add this variable to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"extinction_az\"]=df[[pathdifflist]].min(axis=1) # new variable is azimuth at minimum path difference --> roughly indicates azimuth angle (interpolation necessary=\n",
    "# use .loc to get the index of the column of min and to select corresponding azimuth column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a new reduced df with only label, shape, etc.,  wavelength and corresponding angle of brightest and darkest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate polar angle and add to reduced df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne\t= 1.310893\n",
    "sample_temp = -16.0 # deg C\n",
    "nw =1.309-(11.4+3.84*sample_temp)*0.00001\n",
    "largest_sample_path_diff = df2[\"path_diff\"].max() # nm\n",
    "sample_thickness = (largest_sample_path_diff / (ne - nw))*0.000000001*1000 # mm\n",
    "df2[\"ne_prime\"]= (df2[\"path_diff\"]/sample_thickness)*0.000001 + nw\n",
    "df2[\"polar_angle\"] = np.arcsin((((nw/df2[\"ne_prime\"])**2.-1.)/((nw/ne)**2.-1.))**0.5)*180./np.pi\n",
    "print(f\"max_pd {largest_sample_path_diff:.0f}, thickness {sample_thickness:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot polar angle, maybe azimuth, report average and std dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage=plt.hist(df2[\"area\"], bins=20)\n",
    "print(\"Average Area and StdDev\")\n",
    "print(df2[\"area\"].mean(), np.std(df2[\"area\"]))\n",
    "print(\"Number of valid crystals\")\n",
    "print(len(df2[\"polar_angle\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the axes\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax  = fig.add_subplot(111)\n",
    "# now plot\n",
    "data=df2[\"polar_angle\"]\n",
    "myHist = ax.hist(data, 20)\n",
    "avg=np.linspace(0,int(myHist[0].max()))\n",
    "x = np.zeros(50)+60.\n",
    "h = ax.plot(x, avg, lw=2)\n",
    "# show\n",
    "plt.show()\n",
    "print(\"Average Polar Angle and StdDev\")\n",
    "print(df2[\"polar_angle\"].mean(), np.std(df2[\"polar_angle\"]))\n",
    "print(\"Number of valid crystals\")\n",
    "print(len(df2[\"polar_angle\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are no path differences measured between 410-650 and 730-max (exclusive) and therefore gaps in the angles, too, esp. 50-60 (70) degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df2[\"path_diff\"],df2[\"polar_angle\"],marker=\".\", linestyle=\"none\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Polar angles of 42 +/- 11 (302) and 38 +/- 10 (511) with sample thickness of and 0.36 and 0.29 mm versus a random expectation of 60 degrees\n",
    "- 1993 result for this sample was 47 +/- 16 (157) at 0.30 mm\n",
    "<br><br>\n",
    "\n",
    "- Detecting edges first, then thresholding, then segmenting with \"connected components\" relies on a good thresholding filter\n",
    "- The HED model is sensitive to the contrast in the input image\n",
    "- The combination of HED/connected components found a high number of very small crystals -- at least the edge detection identified them as small (thick edge boundaries took away pixels)\n",
    "- The microbiology-trained cellpose model finds fewer very small shapes but more individual segments\n",
    "- The labelling in cellpose is different in each image and could be tracked with a bit more work\n",
    "- However aggregating the cellpose results across images is not straightforward, so you choose the best one <br><br>\n",
    "\n",
    "- Both methods harvested many many more crystals with less work than by hand\n",
    "- It's likely that both ML methods overlooked the darkest crystals, which happen to be of highest importance\n",
    "- It's likely that the human method over-sampled the dark crystals, because they're the most important!\n",
    "- The ML methods however both found a lower average polar angle which is statistically significantly different from the expected random distribution\n",
    "- The human method found a larger polar angle which was not distinguishable from random\n",
    "- This result is due to the sampling of the crystals <br><br>\n",
    "\n",
    "- Matching path differences from the color chart in the difference algorithm works for pixels chosen from the chart, but does it really match the sample images well?\n",
    "- The yellows in the data seem to match the yellows in the color chart beyond 750 nm but *there is no blue at all* in the sample so the yellows in the sample are actually bronze and to the left of the blue band in the color chart (smaller path difference) which is how I identified them in 1993 as well.\n",
    "- Not limiting the Michel-Levy search to order 1 increases variance as some matches fit higher orders\n",
    "- Samples thicker than order 1 will cause difficulty with this method (using average pixel values in a ROI cannot discern order) \n",
    "- Some versions of the Michel-Levy chart show brighter yellows in the smaller path differences ... a matter of (human) judgment\n",
    "- Using the same CCD/software to calibrate a digital interference chart and record the sample's colors would help this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,100))\n",
    "ax[0].imshow(spectral_img, extent=[0, 1796, 0, 20*spectral_img.shape[0]])\n",
    "ax[0].plot(xx, 0*xx, '-', linewidth=0, color=\"black\")\n",
    "ax[0].set_xlabel(\"Path Difference (nm)\")\n",
    "ax[0].set_xlim(0,1800)\n",
    "\n",
    "\n",
    "ax[1].imshow(cv2.cvtColor(im_list[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do snow crystals point straight up after you ski over them? Think about it next time you go skiing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastimage=plt.imread(\"../project/Colbeck_Equation.jpg\")\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(lastimage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "## Ice physics and mineralogy\n",
    "Colbeck, S.C. Friction of Sliders on Snow. CRREL 92-2. 1992.\n",
    "<br>\n",
    "Hobbs. P.V. Ice Physics. Clarendon Press. Oxford. 1964.\n",
    "<br>\n",
    "Ackerson, B.J. and N.A. Clark. \"Shear-Induced Melting.\" Phys. Rev. Letters V46#2 1/12/81 pp.123-126.\n",
    "<br>\n",
    "Various contributors. Geological Digressions, Optical Mineralogy, some terminology at  https://www.geological-digressions.com/optical-mineralogy-some-terminology/ (03.2023)\n",
    "<br>\n",
    "Nikon camera. Microscopy. https://www.microscopyu.com/techniques/polarized-light/principles-of-birefringence (03.2023)\n",
    "<br><br>\n",
    "## Michel-Lévy color chart of interference colors\n",
    "Delly, J.G. The Michel-Lévy Interference Color Chart- Microscopy's Magical Color Key. https://www.mccrone.com/mm/the-michel-levy-interference-color-chart/\n",
    "<br><br>\n",
    "## Python to convert RGB into wavelength\n",
    "<br>\n",
    "https://stackoverflow.com/questions/71977306/how-to-convert-rgb-to-wavelength-in-python\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Dominant_wavelength\n",
    "<br>\n",
    "https://scipython.com/blog/converting-a-spectrum-to-a-colour/\n",
    "<br>\n",
    "https://stackoverflow.com/questions/5817474/how-to-get-the-wavelength-of-a-pixel-using-rgb\n",
    "<br><br>\n",
    "## Computer Vision\n",
    "<br>\n",
    "Lucas von Chamier*, Romain F. Laine*, Johanna Jukkala, Christoph Spahn, Daniel Krentzel, Elias Nehme, Martina Lerche, Sara Hernández-pérez, Pieta Mattila, Eleni Karinou, Séamus Holden, Ahmet Can Solak, Alexander Krull, Tim-Oliver Buchholz, Martin L Jones, Loic Alain Royer, Christophe Leterrier, Yoav Shechtman, Florian Jug, Mike Heilemann, Guillaume Jacquemet, Ricardo Henriques. Democratising deep learning for microscopy with ZeroCostDL4Mic. Nature Communications, 2021. DOI: https://doi.org/10.1038/s41467-021-22518-0\n",
    "<br>\n",
    "Saining Xie, Zhuowen Tu,Holistically-Nested Edge Detection, Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015, pp. 1395-1403 \n",
    "<br>\n",
    "Pachitariu, M. & Stringer, C. (2022). Cellpose 2.0: how to train your own model. Nature methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CASMLCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
